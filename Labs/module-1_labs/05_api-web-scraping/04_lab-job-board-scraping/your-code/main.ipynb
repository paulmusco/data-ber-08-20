{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analytics Associate, CrossInstall</td>\n",
       "      <td>General Mills</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist (Remote)</td>\n",
       "      <td>Kraken Digital Asset Exchange</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DATA SCIENTIST I</td>\n",
       "      <td>The Home Depot</td>\n",
       "      <td>Houston, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Junior Data Scientist Apprenticeship</td>\n",
       "      <td>IBM</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science - Intern</td>\n",
       "      <td>Sonde Health, Inc.</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>Curology</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - Chicago</td>\n",
       "      <td>SpiderRock</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist 1</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>San Jose, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Financial Analytics Consultant</td>\n",
       "      <td>Toyota North America</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Research Assistant IV Non-Lab (Research Data A...</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Analyst, Data and Analysis</td>\n",
       "      <td>Digitas India</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>CapTech Ventures, Inc</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Analyst Intern</td>\n",
       "      <td>Global Atlantic Financial Group</td>\n",
       "      <td>Des Moines, IA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Shoe Carnival, Inc.</td>\n",
       "      <td>Columbia, SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Quantitative Researcher</td>\n",
       "      <td>Pantera Capital</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>PerBlue</td>\n",
       "      <td>Madison, WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fraud Intelligence, Data Operations Analyst</td>\n",
       "      <td>White Ops</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Developer/Analyst</td>\n",
       "      <td>Thrivent</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Science and Analytics</td>\n",
       "      <td>Thermo Fisher Scientific</td>\n",
       "      <td>Carlsbad, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Renaissance Learning</td>\n",
       "      <td>Madison, WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Analytics Consultant</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Washington, DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>UHNW Business Analytics</td>\n",
       "      <td>Mstream</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Analyst, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Aditi Consulting</td>\n",
       "      <td>Chandler, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>Koch Industries</td>\n",
       "      <td>Louisville, CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0              Data Analytics Associate, CrossInstall   \n",
       "1                             Data Scientist (Remote)   \n",
       "2                                    DATA SCIENTIST I   \n",
       "3                Junior Data Scientist Apprenticeship   \n",
       "4                               Data Science - Intern   \n",
       "5                                 Data Science Intern   \n",
       "6                              Data Analyst - Chicago   \n",
       "7                                    Data Scientist 1   \n",
       "8                      Financial Analytics Consultant   \n",
       "9   Research Assistant IV Non-Lab (Research Data A...   \n",
       "10                         Analyst, Data and Analysis   \n",
       "11                                       Data Analyst   \n",
       "12                                Data Analyst Intern   \n",
       "13                                       Data Analyst   \n",
       "14                            Quantitative Researcher   \n",
       "15                                       Data Analyst   \n",
       "16        Fraud Intelligence, Data Operations Analyst   \n",
       "17                             Data Developer/Analyst   \n",
       "18                         Data Science and Analytics   \n",
       "19                                       Data Analyst   \n",
       "20                               Analytics Consultant   \n",
       "21                            UHNW Business Analytics   \n",
       "22                         Analyst, Data and Analysis   \n",
       "23                                       Data Analyst   \n",
       "24                                     DATA SCIENTIST   \n",
       "\n",
       "                            Company                         Location  \n",
       "0                     General Mills                San Francisco, CA  \n",
       "1     Kraken Digital Asset Exchange                      Chicago, IL  \n",
       "2                    The Home Depot                      Houston, TX  \n",
       "3                               IBM                     New York, NY  \n",
       "4                Sonde Health, Inc.                       Boston, MA  \n",
       "5                          Curology                San Francisco, CA  \n",
       "6                        SpiderRock                      Chicago, IL  \n",
       "7                            PayPal                     San Jose, CA  \n",
       "8              Toyota North America                       Dallas, TX  \n",
       "9                Harvard University                       Boston, MA  \n",
       "10                    Digitas India                       Boston, MA  \n",
       "11            CapTech Ventures, Inc                    Charlotte, NC  \n",
       "12  Global Atlantic Financial Group                   Des Moines, IA  \n",
       "13              Shoe Carnival, Inc.                     Columbia, SC  \n",
       "14                  Pantera Capital                San Francisco, CA  \n",
       "15                          PerBlue                      Madison, WI  \n",
       "16                        White Ops  New York City Metropolitan Area  \n",
       "17                         Thrivent                  Minneapolis, MN  \n",
       "18         Thermo Fisher Scientific                     Carlsbad, CA  \n",
       "19             Renaissance Learning                      Madison, WI  \n",
       "20                         Deloitte                   Washington, DC  \n",
       "21                          Mstream                     New York, NY  \n",
       "22            Digitas North America                       Boston, MA  \n",
       "23                 Aditi Consulting                     Chandler, AZ  \n",
       "24                  Koch Industries                   Louisville, CO  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search(keywords, num_pages): \n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Define new parameter\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    \n",
    "    # Assemble the full url with parameters (UPDATED)\n",
    "    for page in range(num_pages):\n",
    "        if page != 0:\n",
    "            page *= 25\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&start=', str(page)])\n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "        data = pd.DataFrame({\"title\": titles,\n",
    "                            \"company\": companies,\n",
    "                             \"location\": locations\n",
    "        })\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Analyst Level-1</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst Level-1</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Bowie, MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Analyst Fresher</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Framingham, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urgent - Junior Level Business Analyst</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Process and Task mining Analyst</td>\n",
       "      <td>Zealogics LLC</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>DATA ANALYST INTERN</td>\n",
       "      <td>CARE</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Instapage</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Data Analyst - Remote Position! - Dallas</td>\n",
       "      <td>Seasoned Recruitment</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Quantitative Trading Analyst Intern</td>\n",
       "      <td>DRW</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Massachusetts General Hospital</td>\n",
       "      <td>Charlestown, NH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title                         company  \\\n",
       "0                   Business Analyst Level-1                        Staffigo   \n",
       "1                   Business Analyst Level-1                        Staffigo   \n",
       "2                   Business Analyst Fresher                        Staffigo   \n",
       "3     Urgent - Junior Level Business Analyst                        Staffigo   \n",
       "4            Process and Task mining Analyst                   Zealogics LLC   \n",
       "..                                       ...                             ...   \n",
       "70                       DATA ANALYST INTERN                            CARE   \n",
       "71                              Data Analyst                       Instapage   \n",
       "72  Data Analyst - Remote Position! - Dallas            Seasoned Recruitment   \n",
       "73       Quantitative Trading Analyst Intern                             DRW   \n",
       "74                            Data Analyst I  Massachusetts General Hospital   \n",
       "\n",
       "             location  \n",
       "0       Cambridge, MA  \n",
       "1           Bowie, MD  \n",
       "2      Framingham, MA  \n",
       "3          Boston, MA  \n",
       "4        New York, NY  \n",
       "..                ...  \n",
       "70        Atlanta, GA  \n",
       "71  San Francisco, CA  \n",
       "72         Dallas, TX  \n",
       "73        Chicago, IL  \n",
       "74    Charlestown, NH  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data%20analysis', 3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search(keywords, num_pages, country): \n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Define new parameter\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    \n",
    "    # Assemble the full url with parameters (UPDATED)\n",
    "    for page in range(num_pages):\n",
    "        if page != 0:\n",
    "            page *= 25\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&location=', country, '&start=', str(page)])\n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "        data = pd.DataFrame({\"title\": titles,\n",
    "                            \"company\": companies,\n",
    "                             \"location\": locations\n",
    "        })\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masterarbeit Statistics &amp; Data Analysis</td>\n",
       "      <td>Boehringer Ingelheim</td>\n",
       "      <td>Biberach, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyst, Data Management and Analysis (Remote)</td>\n",
       "      <td>Tent Partnership for Refugees</td>\n",
       "      <td>Frankfurt am Main, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Human Capital Advisory Group</td>\n",
       "      <td>Stuttgart Region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Junior) Data Analyst *</td>\n",
       "      <td>myToys.de G.m.b.H</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data and Insights Analyst</td>\n",
       "      <td>TheSoul Publishing</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Data Analyst / Data Engineer (m/w/d)</td>\n",
       "      <td>uptodate Ventures GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Data Analyst / Vertriebscontrolling / Data Sci...</td>\n",
       "      <td>Campusjäger</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Junior Data Analyst (m/w/d)</td>\n",
       "      <td>Computer Futures</td>\n",
       "      <td>Hamburg, Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Data Analyst (f/m/x)</td>\n",
       "      <td>audibene</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Data Analyst (f/m/d)</td>\n",
       "      <td>rubarb</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0             Masterarbeit Statistics & Data Analysis   \n",
       "1      Analyst, Data Management and Analysis (Remote)   \n",
       "2                                        Data Analyst   \n",
       "3                             (Junior) Data Analyst *   \n",
       "4                          Data and Insights Analyst    \n",
       "..                                                ...   \n",
       "70               Data Analyst / Data Engineer (m/w/d)   \n",
       "71  Data Analyst / Vertriebscontrolling / Data Sci...   \n",
       "72                        Junior Data Analyst (m/w/d)   \n",
       "73                               Data Analyst (f/m/x)   \n",
       "74                               Data Analyst (f/m/d)   \n",
       "\n",
       "                          company                               location  \n",
       "0            Boehringer Ingelheim   Biberach, Baden-Württemberg, Germany  \n",
       "1   Tent Partnership for Refugees      Frankfurt am Main, Hesse, Germany  \n",
       "2    Human Capital Advisory Group                       Stuttgart Region  \n",
       "3               myToys.de G.m.b.H                Berlin, Berlin, Germany  \n",
       "4              TheSoul Publishing                                Germany  \n",
       "..                            ...                                    ...  \n",
       "70         uptodate Ventures GmbH               Munich, Bavaria, Germany  \n",
       "71                    Campusjäger  Stuttgart, Baden-Württemberg, Germany  \n",
       "72               Computer Futures              Hamburg, Hamburg, Germany  \n",
       "73                       audibene                Berlin, Berlin, Germany  \n",
       "74                         rubarb                       Hamburg, Germany  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data%20analysis', 3, 'Germany')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "def scrape_linkedin_job_search(keywords, num_pages, country, num_days): \n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Define new parameter\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    seconds = 86400 * num_days\n",
    "    \n",
    "    # Assemble the full url with parameters (UPDATED)\n",
    "    for page in range(num_pages):\n",
    "        if page != 0:\n",
    "            page *= 25\n",
    "        scrape_url = ''.join([BASE_URL, 'f_TPR=r', str(seconds), 'keywords=', keywords, '&location=', country, '&start=', str(page)])\n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "   \n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "        data = pd.DataFrame({\"title\": titles,\n",
    "                            \"company\": companies,\n",
    "                             \"location\": locations\n",
    "        })\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remote Data Entry Clerk - Work at Home</td>\n",
       "      <td>WW-Recruit</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modeling Engineer (d/f/m)</td>\n",
       "      <td>Henkel</td>\n",
       "      <td>Düsseldorf, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Account Executive</td>\n",
       "      <td>IG Recruit</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Managing Director</td>\n",
       "      <td>ACL Partners</td>\n",
       "      <td>Frankfurt, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Director Business Innovation Consulting (all g...</td>\n",
       "      <td>Zühlke Group</td>\n",
       "      <td>Stuttgart Region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Digital Merchandising Assistant</td>\n",
       "      <td>SharkNinja</td>\n",
       "      <td>Frankfurt am Main, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Sales And Marketing Specialist</td>\n",
       "      <td>Do-Techs UG</td>\n",
       "      <td>Constance, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Financial Controller</td>\n",
       "      <td>expand executive search</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Head Of Marketing</td>\n",
       "      <td>AZUM system</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Account Manager (Food sector)</td>\n",
       "      <td>Catenon</td>\n",
       "      <td>Greater Leipzig Area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0              Remote Data Entry Clerk - Work at Home   \n",
       "1                           Modeling Engineer (d/f/m)   \n",
       "2                                   Account Executive   \n",
       "3                                   Managing Director   \n",
       "4   Director Business Innovation Consulting (all g...   \n",
       "..                                                ...   \n",
       "70                    Digital Merchandising Assistant   \n",
       "71                     Sales And Marketing Specialist   \n",
       "72                               Financial Controller   \n",
       "73                                  Head Of Marketing   \n",
       "74                      Account Manager (Food sector)   \n",
       "\n",
       "                    company                                     location  \n",
       "0                WW-Recruit                             Hamburg, Germany  \n",
       "1                    Henkel  Düsseldorf, North Rhine-Westphalia, Germany  \n",
       "2                IG Recruit                     Munich, Bavaria, Germany  \n",
       "3              ACL Partners                    Frankfurt, Hesse, Germany  \n",
       "4              Zühlke Group                             Stuttgart Region  \n",
       "..                      ...                                          ...  \n",
       "70               SharkNinja            Frankfurt am Main, Hesse, Germany  \n",
       "71              Do-Techs UG        Constance, Baden-Württemberg, Germany  \n",
       "72  expand executive search                     Munich, Bavaria, Germany  \n",
       "73              AZUM system                                      Germany  \n",
       "74                  Catenon                         Greater Leipzig Area  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data%20analysis', 3, 'Germany', 1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
